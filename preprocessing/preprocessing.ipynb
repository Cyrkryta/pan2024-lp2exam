{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(default_directory: str):\n",
    "    \"\"\"\n",
    "    Preprocess the data and labels to provide text pair\n",
    "\n",
    "    Args:\n",
    "        default_directory: Default directory for both training and validation data\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary contained processed training and validation sets\n",
    "    \"\"\"\n",
    "\n",
    "    # Defining dictionary\n",
    "    data_dict = {\n",
    "        \"train\": [],\n",
    "        \"validation\": []\n",
    "    }\n",
    "\n",
    "    # Iterate through folders\n",
    "    for split in [\"train\", \"validation\"]:\n",
    "        for difficulty in [\"easy\", \"medium\", \"hard\"]:\n",
    "            # Difficulty dict\n",
    "            difficulty_dict = os.path.join(default_directory, difficulty)\n",
    "            # Set current directory [train, validation]\n",
    "            current_directory = os.path.join(difficulty_dict, split)\n",
    "            \n",
    "            # Iterate over all filenames\n",
    "            for filename in os.listdir(current_directory):\n",
    "                # Only work on .txt files\n",
    "                if filename.endswith(\".txt\"):\n",
    "                    text_path = os.path.join(current_directory, filename)\n",
    "                    label_path = os.path.join(current_directory, \"truth-\" + filename.replace(\".txt\", \".json\"))\n",
    "\n",
    "                    # Open an process the files\n",
    "                    # Text files\n",
    "                    with open(text_path) as f:\n",
    "                        text = f.read()\n",
    "                    paragraphs = text.strip().split(\"\\n\")\n",
    "                    # Labels\n",
    "                    with open(label_path) as f:\n",
    "                        object = json.load(f)\n",
    "                    labels = object.get(\"changes\")\n",
    "                    \n",
    "                    # print(paragraphs)\n",
    "                    # print(labels)\n",
    "\n",
    "                    # Error handling by removing badly formatted files\n",
    "                    if len(labels) != len(paragraphs)-1:\n",
    "                        os.remove(text_path)\n",
    "                        os.remove(label_path)\n",
    "                        print(\"Removed bad formatted files\")\n",
    "                    else: \n",
    "                        # Fill up data_dict\n",
    "                        for i in range(1, len(paragraphs)):\n",
    "                            data_dict[split].append([paragraphs[i-1], paragraphs[i], labels[i-1], difficulty])\n",
    "    return data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed bad formatted files\n",
      "Removed bad formatted files\n",
      "Removed bad formatted files\n",
      "Removed bad formatted files\n",
      "Removed bad formatted files\n",
      "Removed bad formatted files\n",
      "51962\n",
      "11194\n"
     ]
    }
   ],
   "source": [
    "default_directory = \"../pan24-multi-author-analysis\"\n",
    "data = create_datasets(default_directory=default_directory)\n",
    "train_df = pd.DataFrame(data.get(\"train\"), columns=[\"paragraph1\", \"paragraph2\", \"label\", \"difficulty\"])\n",
    "validation_df = pd.DataFrame(data.get(\"validation\"), columns=[\"paragraph1\", \"paragraph2\", \"label\", \"difficulty\"])\n",
    "print(len(train_df))\n",
    "print(len(validation_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('../FINAL-DATA/train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebalance_data(df, difficulty_col='difficulty',label_col='label'):\n",
    "    dfs_balanced = []\n",
    "\n",
    "    for difficulty in df[difficulty_col].unique():\n",
    "        df_difficulty = df[df[difficulty_col] == difficulty]\n",
    "        label_count = df_difficulty.groupby(label_col)[label_col].count()\n",
    "        min_count = label_count.min()\n",
    "        df_balanced = df_difficulty.groupby(label_col).apply(lambda x: x.sample(min_count)).reset_index(drop=True)\n",
    "        dfs_balanced.append(df_balanced)\n",
    "\n",
    "    balanced_df = pd.concat(dfs_balanced).reset_index(drop=True)\n",
    "    return balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5599\n",
      "5595\n"
     ]
    }
   ],
   "source": [
    "def create_test_set(df, n_samples_per_difficulty=1865):\n",
    "    test_samples = []\n",
    "    for difficulty in df['difficulty'].unique():\n",
    "        df_difficulty = df[df['difficulty'] == difficulty]\n",
    "        sample = df_difficulty.sample(n=n_samples_per_difficulty, random_state=42)\n",
    "        test_samples.append(sample)\n",
    "    \n",
    "    test_df = pd.concat(test_samples).reset_index(drop=True)\n",
    "    df = df.drop(test_df.index).reset_index(drop=True)\n",
    "\n",
    "    return df, test_df\n",
    "\n",
    "validation_df, test_df = create_test_set(validation_df)\n",
    "print(len(validation_df))\n",
    "print(len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df.to_csv('../FINAL-DATA/validation.csv', index=False)\n",
    "test_df.to_csv('../FINAL-DATA/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balanced_train_df = rebalance_data(df=train_df)\n",
    "# balanced_validation_df = rebalance_data(df=validation_df)\n",
    "# print(len(balanced_train_df))\n",
    "# print(len(balanced_validation_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balanced_train_df.to_csv('./FINAL-DATA/train_balanced.csv', index=False)\n",
    "# balanced_validation_df.to_csv('./FINAL-DATA/validation_balanced.csv', index=False)\n",
    "# test_df.to_csv('./FINAL-DATA/test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "writing-style-detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
