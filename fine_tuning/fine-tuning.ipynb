{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "lr = 2e-5\n",
    "epochs = 3\n",
    "checkpoint = \"distilbert-base-cased\"\n",
    "\n",
    "\n",
    "train_sets = ['../data/train_cat.csv', '../data/train.csv', '../data/train_aug.csv', '../data/train_augcat.csv',  '../data/train.csv', '../data/train_aug.csv', '../data/train_augcat.csv', '../data/train_cat.csv']\n",
    "test_sets = [  '../data/test_cat.csv', '../data/test.csv', '../data/test.csv', '../data/test_cat.csv', '../data/test.csv', '../data/test.csv', '../data/test_cat.csv', '../data/test_cat.csv']\n",
    "validation_sets = ['../data/val_cat.csv', '../data/validation.csv', '../data/validation.csv', '../data/val_cat.csv', '../data/validation.csv', '../data/validation.csv', '../data/val_cat.csv', '../data/val_cat.csv']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "def create_data(train_p, val_p, test_p):\n",
    "        \n",
    "    df_train = pd.read_csv(train_p)\n",
    "\n",
    "    df_val = pd.read_csv(val_p)\n",
    "    # df_val = df_val.loc[df_val['difficulty'].isin(['medium', 'hard'])]\n",
    "\n",
    "    df_test = pd.read_csv(test_p)#\n",
    "\n",
    "    df_test_easy = df_test[df_test[\"difficulty\"] == \"easy\"]\n",
    "    df_test_medium = df_test[df_test[\"difficulty\"] == \"medium\"]\n",
    "    df_test_hard = df_test[df_test[\"difficulty\"] == \"hard\"]    \n",
    "\n",
    "    raw_datasets = None\n",
    "    \n",
    "    if \"cat\" in train_p:\n",
    "        columns = [\"paragraph\", \"label\"]\n",
    "\n",
    "        # Creating raw dataset\n",
    "        raw_datasets = DatasetDict({\n",
    "            \"train\": Dataset.from_dict({\n",
    "                \"paragraph\": df_train[\"paragraph\"],\n",
    "                \"label\": df_train[\"label\"]\n",
    "            }),\n",
    "            \"validation\": Dataset.from_dict({\n",
    "                \"paragraph\": df_val[\"paragraph\"],\n",
    "                \"label\": df_val[\"label\"]\n",
    "            }),\n",
    "            \"test_easy\": Dataset.from_dict({\n",
    "                \"paragraph\": df_test_easy[\"paragraph\"],\n",
    "                \"label\": df_test_easy[\"label\"],\n",
    "            }),\n",
    "            \"test_medium\": Dataset.from_dict({\n",
    "                \"paragraph\": df_test_medium[\"paragraph\"],\n",
    "                \"label\": df_test_medium[\"label\"],\n",
    "            }),\n",
    "            \"test_hard\": Dataset.from_dict({\n",
    "                \"paragraph\": df_test_hard[\"paragraph\"],\n",
    "                \"label\": df_test_hard[\"label\"],\n",
    "            }),\n",
    "        })\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Defining column names\n",
    "        columns = [\"paragraph1\", \"paragraph2\", \"label\"]\n",
    "\n",
    "        # Creating raw dataset\n",
    "        raw_datasets = DatasetDict({\n",
    "            \"train\": Dataset.from_dict({\n",
    "                \"paragraph1\": df_train[\"paragraph1\"],\n",
    "                \"paragraph2\": df_train[\"paragraph2\"],\n",
    "                \"label\": df_train[\"label\"]\n",
    "            }),\n",
    "            \"validation\": Dataset.from_dict({\n",
    "                \"paragraph1\": df_val[\"paragraph1\"],\n",
    "                \"paragraph2\": df_val[\"paragraph2\"],\n",
    "                \"label\": df_val[\"label\"]\n",
    "            }),\n",
    "            \"test_easy\": Dataset.from_dict({\n",
    "                \"paragraph1\": df_test_easy[\"paragraph1\"],\n",
    "                \"paragraph2\": df_test_easy[\"paragraph2\"],\n",
    "                \"label\": df_test_easy[\"label\"],\n",
    "            }),\n",
    "            \"test_medium\": Dataset.from_dict({\n",
    "                \"paragraph1\": df_test_medium[\"paragraph1\"],\n",
    "                \"paragraph2\": df_test_medium[\"paragraph2\"],\n",
    "                \"label\": df_test_medium[\"label\"],\n",
    "            }),\n",
    "            \"test_hard\": Dataset.from_dict({\n",
    "                \"paragraph1\": df_test_hard[\"paragraph1\"],\n",
    "                \"paragraph2\": df_test_hard[\"paragraph2\"],\n",
    "                \"label\": df_test_hard[\"label\"],\n",
    "            }),\n",
    "        })\n",
    "    \n",
    "    return raw_datasets\n",
    "\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def tokenize_encode(checkpoint, raw_datasets, is_cat=False):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "    if is_cat:\n",
    "        def tokenize_function(sample):\n",
    "            return tokenizer(\n",
    "                sample[\"paragraph\"],\n",
    "                truncation=True\n",
    "            )\n",
    "    else:\n",
    "        def tokenize_function(sample):\n",
    "            return tokenizer(\n",
    "                sample[\"paragraph1\"],\n",
    "                sample[\"paragraph2\"],\n",
    "                truncation=True\n",
    "            )\n",
    "\n",
    "    tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "    collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    if is_cat:\n",
    "        for key in tokenized_datasets.keys():\n",
    "            tokenized_datasets[key] = tokenized_datasets[key].remove_columns([\"paragraph\"])\n",
    "            tokenized_datasets[key] = tokenized_datasets[key].rename_column(\"label\", \"labels\")\n",
    "            tokenized_datasets[key] = tokenized_datasets[key].with_format(\"torch\")\n",
    "    else:\n",
    "        for key in tokenized_datasets.keys():\n",
    "            tokenized_datasets[key] = tokenized_datasets[key].remove_columns([\"paragraph1\", \"paragraph2\"])\n",
    "            tokenized_datasets[key] = tokenized_datasets[key].rename_column(\"label\", \"labels\")\n",
    "            tokenized_datasets[key] = tokenized_datasets[key].with_format(\"torch\")\n",
    "\n",
    "    \n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        tokenized_datasets[\"train\"], shuffle=True, batch_size=8, collate_fn=collator\n",
    "    )\n",
    "\n",
    "    eval_dataloader = DataLoader(\n",
    "        tokenized_datasets[\"validation\"], batch_size=8, collate_fn=collator\n",
    "    )\n",
    "\n",
    "    test_easy_loader = DataLoader(\n",
    "        tokenized_datasets[\"test_easy\"], batch_size=8, collate_fn=collator\n",
    "    )\n",
    "    test_medium_loader = DataLoader(\n",
    "        tokenized_datasets[\"test_medium\"], batch_size=8, collate_fn=collator\n",
    "    )\n",
    "    test_hard_loader = DataLoader(\n",
    "        tokenized_datasets[\"test_hard\"], batch_size=8, collate_fn=collator\n",
    "    )\n",
    "\n",
    "    return train_dataloader, eval_dataloader, test_easy_loader, test_medium_loader, test_hard_loader\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train_loop(model, train_dataloader, num_training_steps, num_epochs, device, optimizer, lr_scheduler):\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in train_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "    \n",
    "    return model\n",
    "\n",
    "import evaluate\n",
    "\n",
    "def eval(model, eval_dataloader, device):\n",
    "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "    model.eval()\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "    return metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83d053144fdf4213bc22d946f2a9bd1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38574 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d8c987a77c34f11bd191a27869e2dd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5122 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a65aa8e6a1843bfa87fd3992efc498a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1865 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d33d354a87894e008e928cdc94ec3b8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1865 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6ccc51159a947538e8a8f5b13a06c8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1865 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/jarl/anaconda3/envs/torch/lib/python3.12/site-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e5d6f3216954204a5e520a663492305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14466 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for pretrained/MH-cat\n",
      "easy: {'accuracy': 0.7753351206434317, 'f1': 0.8625778943916038}\n",
      "medium: {'accuracy': 0.7924932975871314, 'f1': 0.810757946210269}\n",
      "hard: {'accuracy': 0.636461126005362, 'f1': 0.6266519823788547}\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55468b18f4694ee8908339ae412ec781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/51962 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c23c3738465d418abbfc5699946ce6b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9218bd1e576b4e0087d35c5b5c00b168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1865 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4406e6cde92d4a08b1dd710a64fe2d1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1865 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22dd0561e99747ada0c49a9faddd24af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1865 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/jarl/anaconda3/envs/torch/lib/python3.12/site-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ef9142b6d90493db53d714d02d7ca6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for pretrained/MHE\n",
      "easy: {'accuracy': 0.9050938337801608, 'f1': 0.9461187214611873}\n",
      "medium: {'accuracy': 0.8091152815013405, 'f1': 0.8309591642924976}\n",
      "hard: {'accuracy': 0.6348525469168901, 'f1': 0.6110793832095945}\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abd3ddf5e0274be4b27a5438bfb6de91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/103924 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "152eb069ba5d47d5b17c922847527599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3255c8629954ea8a232d534b98621cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1865 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f15ed0790a294c978343b00b8a7866da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1865 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d468e483050846b69b58783746a7c2de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1865 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/jarl/anaconda3/envs/torch/lib/python3.12/site-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8de114666764e75b091cec78140e521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38973 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for pretrained/MH-aug\n",
      "easy: {'accuracy': 0.9093833780160858, 'f1': 0.9484598963098506}\n",
      "medium: {'accuracy': 0.7908847184986595, 'f1': 0.8163841807909604}\n",
      "hard: {'accuracy': 0.6343163538873995, 'f1': 0.6333333333333333}\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7764c89b15954173912c1bbc846ba709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/77148 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91d5d4114b4c429ba6757a25a4d7fe64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5122 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6e34e91041e4cb3a67f75f851b797cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1865 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd58862a006044708e458e06789e17d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1865 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48b363d442614e0c9f5a81ac8d751962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1865 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/jarl/anaconda3/envs/torch/lib/python3.12/site-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b48b6174a3e746d8a7388d100cfeaacd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28932 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for pretrained/MH-augcat\n",
      "easy: {'accuracy': 0.7630026809651475, 'f1': 0.853253652058433}\n",
      "medium: {'accuracy': 0.797319034852547, 'f1': 0.8166828322017459}\n",
      "hard: {'accuracy': 0.6203753351206435, 'f1': 0.6105610561056105}\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "621e01d1bd8d446bb59b27201c8db156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/51962 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10c10b16c3ec4f4e8dd59ccdbf5faec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9efa438fbaa04a92b25001dc6e1a74df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1865 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9450578acb534c388a2861b36b7a03ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1865 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b06d9a9eb15944478aeb92e40d8ec6a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1865 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/jarl/anaconda3/envs/torch/lib/python3.12/site-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f46858b7dd264de2a57313262c68a5a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for pretrained/MH\n",
      "easy: {'accuracy': 0.9158176943699732, 'f1': 0.95220700152207}\n",
      "medium: {'accuracy': 0.8037533512064343, 'f1': 0.8271954674220963}\n",
      "hard: {'accuracy': 0.6418230563002681, 'f1': 0.6204545454545455}\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5642e8a4ac554cfd878d8af0a5c0adae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/103924 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52f85cc891d64152bc1edd63bf88fa6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4232e5c0be104c2f9341ab6356e72515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1865 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bd0a0956630412cba2019bc542ab98a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1865 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a57f09c0c63e49ddaec4d3a692db720c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1865 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/jarl/anaconda3/envs/torch/lib/python3.12/site-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a215bb188df84dfda66c6ea3d5f2689d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38973 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for pretrained/MHE-aug\n",
      "easy: {'accuracy': 0.9115281501340483, 'f1': 0.9495567104860899}\n",
      "medium: {'accuracy': 0.7860589812332439, 'f1': 0.8127639605818865}\n",
      "hard: {'accuracy': 0.6230563002680966, 'f1': 0.613948380010983}\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15249e3a1ec94238bdca075231d15830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/77148 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ab6459a960b4b69975b52f7ff1befdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5122 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "970bb136ff12401897a24e71df235f14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1865 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1d25b0aaa734825b66b541ad815f7f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1865 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa3835c6cbd24611b2a9c4ff7d85271e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1865 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/jarl/anaconda3/envs/torch/lib/python3.12/site-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4304c13abac84b979e5ff81a879d2f71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28932 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for pretrained/MHE-augcat\n",
      "easy: {'accuracy': 0.7640750670241286, 'f1': 0.8536260811709914}\n",
      "medium: {'accuracy': 0.7833780160857908, 'f1': 0.8011811023622047}\n",
      "hard: {'accuracy': 0.6176943699731904, 'f1': 0.5969474279253816}\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14dd0d3fa3b14df096cbe62e8a36e896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38574 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "753216a29eb1465d879137da5676fa0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5122 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cad7601bf8449dea8bbb3b93bde4d64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1865 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f798485940154a73832b999ba57982ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1865 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f819aa49eecc4ebdaa5dffca1e592f7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1865 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/jarl/anaconda3/envs/torch/lib/python3.12/site-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f9c34c7c10542c68665bf6f4386c0f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14466 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for pretrained/MHE-cat\n",
      "easy: {'accuracy': 0.7951742627345845, 'f1': 0.875893437296946}\n",
      "medium: {'accuracy': 0.7898123324396783, 'f1': 0.8072763028515241}\n",
      "hard: {'accuracy': 0.6407506702412868, 'f1': 0.624439461883408}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AdamW\n",
    "from transformers import get_scheduler\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "for i in range(len(train_sets)):\n",
    "    is_cat = \"cat\" in train_sets[i]\n",
    "    raw_datasets = create_data(train_sets[i], validation_sets[i], test_sets[i])\n",
    "    train_dataloader, eval_dataloader, test_easy_loader, test_medium_loader, test_hard_loader = tokenize_encode(checkpoint, raw_datasets, is_cat)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2).to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    num_training_steps = epochs * len(train_dataloader)\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_training_steps,\n",
    "    )\n",
    "\n",
    "    model = train_loop(model, train_dataloader, num_training_steps, epochs, device, optimizer, lr_scheduler)\n",
    "\n",
    "    model.save_pretrained(pretrained_model_folder[i])\n",
    "\n",
    "    easy = eval(model, test_easy_loader, device)\n",
    "    medium = eval(model, test_medium_loader, device)\n",
    "    hard = eval(model, test_hard_loader, device)\n",
    "\n",
    "    print(f\"Results for {pretrained_model_folder[i]}\")\n",
    "    print(f\"easy: {easy}\")\n",
    "    print(f\"medium: {medium}\")\n",
    "    print(f\"hard: {hard}\")\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    #clear memory\n",
    "    del model\n",
    "    del optimizer\n",
    "    del lr_scheduler\n",
    "    torch.cuda.empty_cache()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "writing-style-detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
